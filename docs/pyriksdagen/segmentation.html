<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pyriksdagen.segmentation API documentation</title>
<meta name="description" content="Implements the segmentation of the data into speeches and
ultimately into the Parla-Clarin XML format." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pyriksdagen.segmentation</code></h1>
</header>
<section id="section-intro">
<p>Implements the segmentation of the data into speeches and
ultimately into the Parla-Clarin XML format.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Implements the segmentation of the data into speeches and
ultimately into the Parla-Clarin XML format.
&#34;&#34;&#34;
import numpy as np
import pandas as pd
import re, hashlib, copy, os
import progressbar
from os import listdir
from os.path import isfile, join
from lxml import etree
from pyriksdagen.mp import detect_mp
from pyriksdagen.download import get_blocks, fetch_files
from pyriksdagen.utils import infer_metadata
from pyriksdagen.db import filter_db, year_iterator

# Classify paragraph
def classify_paragraph(paragraph, classifier, prior=np.log([0.8, 0.2])):
    &#34;&#34;&#34;
    Classify paragraph into speeches / descriptions with provided classifier

    &#34;&#34;&#34;
    words = paragraph.split()
    V = len(words)
    if V == 0:
        return prior

    x = np.zeros((V, classifier[&#34;dim&#34;]))

    ft = classifier[&#34;ft&#34;]
    for ix, word in enumerate(words):
        vec = ft.get_word_vector(word)
        x[ix] = vec

    pred = classifier[&#34;model&#34;].predict(x, batch_size=V)
    return np.sum(pred, axis=0) + prior

def _is_metadata_block(txt0):
    txt1 = re.sub(&#34;[^a-zA-ZåäöÅÄÖ ]+&#34;, &#34;&#34;, txt0)
    len0 = len(txt0)
    
    # Empty blocks should not be classified as metadata
    if len(txt0.strip()) == 0:
        return False
        
    # Metadata generally don&#39;t introduce other things
    if txt0.strip()[-1] == &#34;:&#34;:
        return False

    # Or list MPs
    if &#34;Anf.&#34; in txt0:
        return False
    
    len1 = len(txt1)
    len2 = len(txt0.strip())
    if len2 == 0:
        return False
    
    # Crude heuristic. Skip if
    # a) over 15% is non alphabetic characters
    # and b) length is under 150 characters
    
    # TODO: replace with ML algorithm
    return float(len1) / float(len0) &lt; 0.85 and len0 &lt; 150

def detect_mp(matched_txt, names_ids, mp_db=None, also_last_name=True):
    &#34;&#34;&#34;
    Match the introduced speaker in a text snippet
    &#34;&#34;&#34;
    person = []

    # Prefer uppercase
    # SVEN LINDGREN
    for name, identifier in names_ids:
        if name.upper() in matched_txt:
            person.append(identifier)

    # Sven Lindgren
    if len(person) == 0:
        for name, identifier in names_ids:
            if name in matched_txt:
                person.append(identifier)

    # Lindgren, Sven
    if len(person) == 0:
        for name, identifier in names_ids:
            last_name = &#34; &#34; + name.split()[-1] + &#34;,&#34;
            if last_name in matched_txt:
                first_name = name.split()[0]
                rest = matched_txt.split(last_name)[-1]
                if first_name in rest:
                    person.append(identifier)

    # TODO: herr Lindgren i Stockholm

    if len(person) == 0 and mp_db is not None:
        for _, row in mp_db.iterrows():
            #print(row)
            i_name = row[&#34;name&#34;].split()[-1] + &#34; &#34; + row[&#34;specifier&#34;]
            #print(i_name)
            if i_name.lower() in matched_txt.lower():
                person.append(row[&#34;id&#34;])

    if also_last_name:
        # Herr Lindgren
        if len(person) == 0:
            matched_txt_lower = matched_txt.lower()
            for name, identifier in names_ids:
                last_name = &#34; &#34; + name.split()[-1]
                herr_name = &#34;herr&#34; + last_name.lower()
                fru_name = &#34;fru&#34; + last_name.lower()

                if herr_name in matched_txt_lower:
                    ix = matched_txt_lower.index(herr_name)
                    aftermatch = matched_txt_lower[ix + len(herr_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

                if fru_name in matched_txt_lower:
                    ix = matched_txt_lower.index(fru_name)
                    aftermatch = matched_txt_lower[ix + len(fru_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

        # Lindgren
        if len(person) == 0:
            for name, identifier in names_ids:
                last_name = &#34; &#34; + name.split()[-1]
                
                if last_name in matched_txt:
                    ix = matched_txt.index(last_name)
                    aftermatch = matched_txt[ix + len(last_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

                elif last_name.upper() in matched_txt:
                    #print(matched_txt, last_name, last_name.upper())
                    person.append(identifier)

    if len(person) == 1:
        return person[0]
    else:
        person_names = list(set([&#34;_&#34;.join(m.split(&#34;_&#34;)[:-1]) for m in person]))
        if len(person_names) == 1:
            return person[-1]
        else:
            return None

def expression_dicts(pattern_db):
    expressions = dict()
    manual = dict()
    for _, row in pattern_db.iterrows():
        if row[&#34;type&#34;] == &#34;regex&#34;:
            pattern = row[&#39;pattern&#39;]
            exp = re.compile(pattern)
            #Calculate digest for distringuishing patterns without ugly characters
            pattern_digest = hashlib.md5(pattern.encode(&#34;utf-8&#34;)).hexdigest()[:16]
            expressions[pattern_digest] = exp
        elif row[&#34;type&#34;] == &#34;manual&#34;:
            manual[row[&#34;pattern&#34;]] = row[&#34;segmentation&#34;]
    return expressions, manual

def detect_introduction(paragraph, expressions, names_ids):
    for pattern_digest, exp in expressions.items():
        for m in exp.finditer(paragraph.strip()):
            matched_txt = m.group()
            person = detect_mp(matched_txt, names_ids)
            segmentation = &#34;speech_start&#34;
            d = {
            &#34;pattern&#34;: pattern_digest,
            &#34;who&#34;: person,
            &#34;segmentation&#34;: segmentation,
            &#34;txt&#34;: matched_txt,
            }

            return d

# Instance detection
def find_instances_xml(root, pattern_db, mp_db, classifier):
    &#34;&#34;&#34;
    Find instances of segment start and end patterns in a txt file.

    Args:
        root: root of an lxml tree to be pattern matched.
        pattern_db: Patterns to be matched as a Pandas DataFrame.
    &#34;&#34;&#34;
    columns = [&#39;protocol_id&#39;, &#34;elem_id&#34;, &#34;pattern&#34;, &#34;segmentation&#34;, &#34;who&#34;, &#34;id&#34;]
    data = []
    protocol_id = root.attrib[&#34;id&#34;]
    metadata = infer_metadata(protocol_id)
    pattern_rows = list(pattern_db.iterrows())
    
    mp_db = mp_db[mp_db[&#34;chamber&#34;] == metadata[&#34;chamber&#34;]]
    names = mp_db[&#34;name&#34;]
    ids = mp_db[&#34;id&#34;]
    names_ids = list(zip(names,ids))
    
    expressions, manual = expression_dicts(pattern_db)
    
    prot_speeches = dict()
    for content_block in root:
        cb_id = content_block.attrib[&#34;id&#34;]
        content_txt = &#39;\n&#39;.join(content_block.itertext())
        if not _is_metadata_block(content_txt):
            for textblock in content_block:
                tb_id = textblock.attrib[&#34;id&#34;]
                paragraph = textblock.text

                # Do not do segmentation if paragraph is empty
                if type(paragraph) != str:
                    continue

                for pattern, segmentation in manual.items():
                    if pattern in paragraph:
                        person = detect_mp(paragraph, names_ids)
                        #person = detect_mp(matched_txt, names_ids)
                        d = {&#34;pattern&#34;: &#34;manual&#34;,
                            &#34;segmentation&#34;: segmentation,
                            &#34;elem_id&#34;: tb_id,
                            }
                        continue

                # Detect speaker introductions
                d = detect_introduction(paragraph, expressions, names_ids)

                # Do not do further segmentation if speech is detected
                if d is not None:
                    d[&#34;elem_id&#34;] = tb_id
                    data.append(d)
                    continue

                # Use ML model to classify paragraph
                if classifier is not None:
                    preds = classify_paragraph(paragraph, classifier)
                    if np.argmax(preds) == 1:
                        segmentation = &#34;note&#34;
                        d = {
                        &#34;segmentation&#34;: segmentation,
                        &#34;elem_id&#34;: tb_id,
                        }

                        data.append(d)
        else:
            d = {&#34;pattern&#34;: None, &#34;who&#34;: None, &#34;segmentation&#34;: &#34;metadata&#34;}
            d[&#34;elem_id&#34;] = cb_id
            data.append(d)

    df = pd.DataFrame(data, columns=columns)
    df[&#34;protocol_id&#34;] = protocol_id
    return df

def apply_instances(protocol, instance_db):
    protocol_id = protocol.attrib[&#34;id&#34;]
    
    applicable_instances = instance_db[instance_db[&#34;protocol_id&#34;] == protocol_id]
    applicable_instances = applicable_instances.drop_duplicates(subset=[&#39;elem_id&#39;])

    for _, row in applicable_instances.iterrows():
        elem_id = row[&#34;elem_id&#34;]
        for target in protocol.xpath(&#34;//*[@id=&#39;&#34; + elem_id + &#34;&#39;]&#34;):
            target.attrib[&#34;segmentation&#34;] = row[&#34;segmentation&#34;]
            if type(row[&#34;who&#34;]) == str:
                target.attrib[&#34;who&#34;] = row[&#34;who&#34;]

            if type(row[&#34;id&#34;]) == str:
                target.attrib[&#34;id&#34;] = row[&#34;id&#34;]

    return protocol
    
def find_instances(protocol_id, archive, pattern_db, mp_db, classifier=None):
    page_content_blocks = get_blocks(protocol_id, archive)
    instance_db = find_instances_xml(page_content_blocks, pattern_db, mp_db, classifier=classifier)
    
    instance_db[&#34;protocol_id&#34;] = protocol_id
    return instance_db
    
def segmentation_workflow(file_db, archive, pattern_db, mp_db, ml=True):
    classifier = None
    if ml:
        import tensorflow as tf
        import fasttext.util

        model = tf.keras.models.load_model(&#34;input/segment-classifier&#34;)

        # Load word vectors from disk or download with the fasttext module
        vector_path = &#39;cc.sv.300.bin&#39;
        fasttext.util.download_model(&#39;sv&#39;, if_exists=&#39;ignore&#39;)
        ft = fasttext.load_model(vector_path)

        classifier = dict(
            model=model,
            ft=ft,
            dim=ft.get_word_vector(&#34;hej&#34;).shape[0]
        )

    instance_dbs = []
    for corpus_year, package_ids, _ in year_iterator(file_db):
        print(&#34;Segmenting year:&#34;, corpus_year)
        
        year_patterns = filter_db(pattern_db, year=corpus_year)
        year_mps = filter_db(mp_db, year=corpus_year)
        print(year_mps)

        for protocol_id in progressbar.progressbar(package_ids):
            protocol_patterns = filter_db(pattern_db, protocol_id=protocol_id)
            protocol_patterns = pd.concat([protocol_patterns, year_patterns])
            instance_db = find_instances(protocol_id, archive, protocol_patterns, year_mps, classifier=classifier)
            instance_dbs.append(instance_db)
    
    return pd.concat(instance_dbs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pyriksdagen.segmentation.apply_instances"><code class="name flex">
<span>def <span class="ident">apply_instances</span></span>(<span>protocol, instance_db)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_instances(protocol, instance_db):
    protocol_id = protocol.attrib[&#34;id&#34;]
    
    applicable_instances = instance_db[instance_db[&#34;protocol_id&#34;] == protocol_id]
    applicable_instances = applicable_instances.drop_duplicates(subset=[&#39;elem_id&#39;])

    for _, row in applicable_instances.iterrows():
        elem_id = row[&#34;elem_id&#34;]
        for target in protocol.xpath(&#34;//*[@id=&#39;&#34; + elem_id + &#34;&#39;]&#34;):
            target.attrib[&#34;segmentation&#34;] = row[&#34;segmentation&#34;]
            if type(row[&#34;who&#34;]) == str:
                target.attrib[&#34;who&#34;] = row[&#34;who&#34;]

            if type(row[&#34;id&#34;]) == str:
                target.attrib[&#34;id&#34;] = row[&#34;id&#34;]

    return protocol</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.classify_paragraph"><code class="name flex">
<span>def <span class="ident">classify_paragraph</span></span>(<span>paragraph, classifier, prior=array([-0.22314355, -1.60943791]))</span>
</code></dt>
<dd>
<div class="desc"><p>Classify paragraph into speeches / descriptions with provided classifier</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classify_paragraph(paragraph, classifier, prior=np.log([0.8, 0.2])):
    &#34;&#34;&#34;
    Classify paragraph into speeches / descriptions with provided classifier

    &#34;&#34;&#34;
    words = paragraph.split()
    V = len(words)
    if V == 0:
        return prior

    x = np.zeros((V, classifier[&#34;dim&#34;]))

    ft = classifier[&#34;ft&#34;]
    for ix, word in enumerate(words):
        vec = ft.get_word_vector(word)
        x[ix] = vec

    pred = classifier[&#34;model&#34;].predict(x, batch_size=V)
    return np.sum(pred, axis=0) + prior</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.detect_introduction"><code class="name flex">
<span>def <span class="ident">detect_introduction</span></span>(<span>paragraph, expressions, names_ids)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_introduction(paragraph, expressions, names_ids):
    for pattern_digest, exp in expressions.items():
        for m in exp.finditer(paragraph.strip()):
            matched_txt = m.group()
            person = detect_mp(matched_txt, names_ids)
            segmentation = &#34;speech_start&#34;
            d = {
            &#34;pattern&#34;: pattern_digest,
            &#34;who&#34;: person,
            &#34;segmentation&#34;: segmentation,
            &#34;txt&#34;: matched_txt,
            }

            return d</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.detect_mp"><code class="name flex">
<span>def <span class="ident">detect_mp</span></span>(<span>matched_txt, names_ids, mp_db=None, also_last_name=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Match the introduced speaker in a text snippet</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_mp(matched_txt, names_ids, mp_db=None, also_last_name=True):
    &#34;&#34;&#34;
    Match the introduced speaker in a text snippet
    &#34;&#34;&#34;
    person = []

    # Prefer uppercase
    # SVEN LINDGREN
    for name, identifier in names_ids:
        if name.upper() in matched_txt:
            person.append(identifier)

    # Sven Lindgren
    if len(person) == 0:
        for name, identifier in names_ids:
            if name in matched_txt:
                person.append(identifier)

    # Lindgren, Sven
    if len(person) == 0:
        for name, identifier in names_ids:
            last_name = &#34; &#34; + name.split()[-1] + &#34;,&#34;
            if last_name in matched_txt:
                first_name = name.split()[0]
                rest = matched_txt.split(last_name)[-1]
                if first_name in rest:
                    person.append(identifier)

    # TODO: herr Lindgren i Stockholm

    if len(person) == 0 and mp_db is not None:
        for _, row in mp_db.iterrows():
            #print(row)
            i_name = row[&#34;name&#34;].split()[-1] + &#34; &#34; + row[&#34;specifier&#34;]
            #print(i_name)
            if i_name.lower() in matched_txt.lower():
                person.append(row[&#34;id&#34;])

    if also_last_name:
        # Herr Lindgren
        if len(person) == 0:
            matched_txt_lower = matched_txt.lower()
            for name, identifier in names_ids:
                last_name = &#34; &#34; + name.split()[-1]
                herr_name = &#34;herr&#34; + last_name.lower()
                fru_name = &#34;fru&#34; + last_name.lower()

                if herr_name in matched_txt_lower:
                    ix = matched_txt_lower.index(herr_name)
                    aftermatch = matched_txt_lower[ix + len(herr_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

                if fru_name in matched_txt_lower:
                    ix = matched_txt_lower.index(fru_name)
                    aftermatch = matched_txt_lower[ix + len(fru_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

        # Lindgren
        if len(person) == 0:
            for name, identifier in names_ids:
                last_name = &#34; &#34; + name.split()[-1]
                
                if last_name in matched_txt:
                    ix = matched_txt.index(last_name)
                    aftermatch = matched_txt[ix + len(last_name):]
                    aftermatch = aftermatch[:1]
                    if aftermatch in [&#34; &#34;, &#34;:&#34;, &#34;,&#34;]:
                        person.append(identifier)

                elif last_name.upper() in matched_txt:
                    #print(matched_txt, last_name, last_name.upper())
                    person.append(identifier)

    if len(person) == 1:
        return person[0]
    else:
        person_names = list(set([&#34;_&#34;.join(m.split(&#34;_&#34;)[:-1]) for m in person]))
        if len(person_names) == 1:
            return person[-1]
        else:
            return None</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.expression_dicts"><code class="name flex">
<span>def <span class="ident">expression_dicts</span></span>(<span>pattern_db)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expression_dicts(pattern_db):
    expressions = dict()
    manual = dict()
    for _, row in pattern_db.iterrows():
        if row[&#34;type&#34;] == &#34;regex&#34;:
            pattern = row[&#39;pattern&#39;]
            exp = re.compile(pattern)
            #Calculate digest for distringuishing patterns without ugly characters
            pattern_digest = hashlib.md5(pattern.encode(&#34;utf-8&#34;)).hexdigest()[:16]
            expressions[pattern_digest] = exp
        elif row[&#34;type&#34;] == &#34;manual&#34;:
            manual[row[&#34;pattern&#34;]] = row[&#34;segmentation&#34;]
    return expressions, manual</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.find_instances"><code class="name flex">
<span>def <span class="ident">find_instances</span></span>(<span>protocol_id, archive, pattern_db, mp_db, classifier=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_instances(protocol_id, archive, pattern_db, mp_db, classifier=None):
    page_content_blocks = get_blocks(protocol_id, archive)
    instance_db = find_instances_xml(page_content_blocks, pattern_db, mp_db, classifier=classifier)
    
    instance_db[&#34;protocol_id&#34;] = protocol_id
    return instance_db</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.find_instances_xml"><code class="name flex">
<span>def <span class="ident">find_instances_xml</span></span>(<span>root, pattern_db, mp_db, classifier)</span>
</code></dt>
<dd>
<div class="desc"><p>Find instances of segment start and end patterns in a txt file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>root</code></strong></dt>
<dd>root of an lxml tree to be pattern matched.</dd>
<dt><strong><code>pattern_db</code></strong></dt>
<dd>Patterns to be matched as a Pandas DataFrame.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_instances_xml(root, pattern_db, mp_db, classifier):
    &#34;&#34;&#34;
    Find instances of segment start and end patterns in a txt file.

    Args:
        root: root of an lxml tree to be pattern matched.
        pattern_db: Patterns to be matched as a Pandas DataFrame.
    &#34;&#34;&#34;
    columns = [&#39;protocol_id&#39;, &#34;elem_id&#34;, &#34;pattern&#34;, &#34;segmentation&#34;, &#34;who&#34;, &#34;id&#34;]
    data = []
    protocol_id = root.attrib[&#34;id&#34;]
    metadata = infer_metadata(protocol_id)
    pattern_rows = list(pattern_db.iterrows())
    
    mp_db = mp_db[mp_db[&#34;chamber&#34;] == metadata[&#34;chamber&#34;]]
    names = mp_db[&#34;name&#34;]
    ids = mp_db[&#34;id&#34;]
    names_ids = list(zip(names,ids))
    
    expressions, manual = expression_dicts(pattern_db)
    
    prot_speeches = dict()
    for content_block in root:
        cb_id = content_block.attrib[&#34;id&#34;]
        content_txt = &#39;\n&#39;.join(content_block.itertext())
        if not _is_metadata_block(content_txt):
            for textblock in content_block:
                tb_id = textblock.attrib[&#34;id&#34;]
                paragraph = textblock.text

                # Do not do segmentation if paragraph is empty
                if type(paragraph) != str:
                    continue

                for pattern, segmentation in manual.items():
                    if pattern in paragraph:
                        person = detect_mp(paragraph, names_ids)
                        #person = detect_mp(matched_txt, names_ids)
                        d = {&#34;pattern&#34;: &#34;manual&#34;,
                            &#34;segmentation&#34;: segmentation,
                            &#34;elem_id&#34;: tb_id,
                            }
                        continue

                # Detect speaker introductions
                d = detect_introduction(paragraph, expressions, names_ids)

                # Do not do further segmentation if speech is detected
                if d is not None:
                    d[&#34;elem_id&#34;] = tb_id
                    data.append(d)
                    continue

                # Use ML model to classify paragraph
                if classifier is not None:
                    preds = classify_paragraph(paragraph, classifier)
                    if np.argmax(preds) == 1:
                        segmentation = &#34;note&#34;
                        d = {
                        &#34;segmentation&#34;: segmentation,
                        &#34;elem_id&#34;: tb_id,
                        }

                        data.append(d)
        else:
            d = {&#34;pattern&#34;: None, &#34;who&#34;: None, &#34;segmentation&#34;: &#34;metadata&#34;}
            d[&#34;elem_id&#34;] = cb_id
            data.append(d)

    df = pd.DataFrame(data, columns=columns)
    df[&#34;protocol_id&#34;] = protocol_id
    return df</code></pre>
</details>
</dd>
<dt id="pyriksdagen.segmentation.segmentation_workflow"><code class="name flex">
<span>def <span class="ident">segmentation_workflow</span></span>(<span>file_db, archive, pattern_db, mp_db, ml=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def segmentation_workflow(file_db, archive, pattern_db, mp_db, ml=True):
    classifier = None
    if ml:
        import tensorflow as tf
        import fasttext.util

        model = tf.keras.models.load_model(&#34;input/segment-classifier&#34;)

        # Load word vectors from disk or download with the fasttext module
        vector_path = &#39;cc.sv.300.bin&#39;
        fasttext.util.download_model(&#39;sv&#39;, if_exists=&#39;ignore&#39;)
        ft = fasttext.load_model(vector_path)

        classifier = dict(
            model=model,
            ft=ft,
            dim=ft.get_word_vector(&#34;hej&#34;).shape[0]
        )

    instance_dbs = []
    for corpus_year, package_ids, _ in year_iterator(file_db):
        print(&#34;Segmenting year:&#34;, corpus_year)
        
        year_patterns = filter_db(pattern_db, year=corpus_year)
        year_mps = filter_db(mp_db, year=corpus_year)
        print(year_mps)

        for protocol_id in progressbar.progressbar(package_ids):
            protocol_patterns = filter_db(pattern_db, protocol_id=protocol_id)
            protocol_patterns = pd.concat([protocol_patterns, year_patterns])
            instance_db = find_instances(protocol_id, archive, protocol_patterns, year_mps, classifier=classifier)
            instance_dbs.append(instance_db)
    
    return pd.concat(instance_dbs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pyriksdagen" href="index.html">pyriksdagen</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pyriksdagen.segmentation.apply_instances" href="#pyriksdagen.segmentation.apply_instances">apply_instances</a></code></li>
<li><code><a title="pyriksdagen.segmentation.classify_paragraph" href="#pyriksdagen.segmentation.classify_paragraph">classify_paragraph</a></code></li>
<li><code><a title="pyriksdagen.segmentation.detect_introduction" href="#pyriksdagen.segmentation.detect_introduction">detect_introduction</a></code></li>
<li><code><a title="pyriksdagen.segmentation.detect_mp" href="#pyriksdagen.segmentation.detect_mp">detect_mp</a></code></li>
<li><code><a title="pyriksdagen.segmentation.expression_dicts" href="#pyriksdagen.segmentation.expression_dicts">expression_dicts</a></code></li>
<li><code><a title="pyriksdagen.segmentation.find_instances" href="#pyriksdagen.segmentation.find_instances">find_instances</a></code></li>
<li><code><a title="pyriksdagen.segmentation.find_instances_xml" href="#pyriksdagen.segmentation.find_instances_xml">find_instances_xml</a></code></li>
<li><code><a title="pyriksdagen.segmentation.segmentation_workflow" href="#pyriksdagen.segmentation.segmentation_workflow">segmentation_workflow</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>